{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wceUYLkzlY5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfopm8ufKktf"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers peft datasets accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Carga del Modelo Base y Tokenizador"
      ],
      "metadata": {
        "id": "DQdlE4Q8KxVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "mvzXVqiDQ-d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Nombre del modelo ligero\n",
        "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "# Cargar modelo base y tokenizador\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",  # Mapea automáticamente a la GPU/CPU\n",
        "    torch_dtype=torch.float16  # Usa half-precision para reducir memoria\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Configurar tokenizador\n",
        "tokenizer.pad_token = \"<pad>\"\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "MkJU3YhfKzva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Configuración de LoRA"
      ],
      "metadata": {
        "id": "giez2ZOsLIlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# Configuración optimizada de LoRA\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=4,  # Dimensión reducida\n",
        "    lora_alpha=8,  # Escalado para más precisión\n",
        "    lora_dropout=0.1,  # Dropout ligero\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Limitar a módulos críticos\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Aplicar LoRA al modelo\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "l5SSqnaTLJ6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de Datos desde JSON"
      ],
      "metadata": {
        "id": "1ivVE8pmM6tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar explícitamente el token <pad> si no está definido correctamente\n",
        "if tokenizer.pad_token_id is None or tokenizer.pad_token != \"<pad>\":\n",
        "    tokenizer.add_special_tokens({'pad_token': '<pad>'})  # Añadir token de padding\n",
        "    model.resize_token_embeddings(len(tokenizer))  # Ajustar el vocabulario del modelo\n",
        "\n",
        "# Configurar el ID del token <pad>\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Validar la configuración\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "print(f\"Pad token ID: {tokenizer.pad_token_id}\")"
      ],
      "metadata": {
        "id": "WLHINl9gCT31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de preprocesamiento\n",
        "def preprocess_function(examples):\n",
        "    # Combinar \"prompt\" y \"response\"\n",
        "    inputs = [f\"{prompt} {response}\" for prompt, response in zip(examples[\"prompt\"], examples[\"response\"])]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        padding=\"max_length\",  # Rellenar con pad_token\n",
        "        truncation=True,       # Cortar a longitud máxima\n",
        "        max_length=256,        # Definir longitud máxima\n",
        "        return_tensors=\"pt\"    # Retornar tensores\n",
        "    )\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone().detach()  # Etiquetas = input_ids\n",
        "    return model_inputs\n",
        "\n",
        "# Aplicar tokenización al dataset\n",
        "tokenized_dataset = split_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "6_n2YQY-M6RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "WWeYwlzKL6EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=4,  # Tamaño de batch mayor porque el modelo es ligero\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    gradient_accumulation_steps=8,\n",
        "    fp16=True,  # Half-precision para reducir tiempo\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# Configurar Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"]\n",
        ")"
      ],
      "metadata": {
        "id": "DxMmkuvjL-BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "NsIBp2s5tcvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardado del Modelo"
      ],
      "metadata": {
        "id": "yVE5dhrzMMTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el modelo ajustado\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")"
      ],
      "metadata": {
        "id": "rnG_uVoGMPZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar el modelo y el tokenizador entrenados"
      ],
      "metadata": {
        "id": "JmAPqq1erFtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Ruta al modelo y tokenizador guardados\n",
        "model_path = \"/content/drive/MyDrive/fine_tuned_model\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Configurar tokenizador\n",
        "tokenizer.pad_token = \"<pad>\"\n",
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "Sa-U165nrG-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probar con un ejemplo de entrada"
      ],
      "metadata": {
        "id": "gm1sEG4XrK0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realizar una predicción de ejemplo\n",
        "test_example = \"Las condiciones del clima y el suelo son las siguientes: Nitrógeno: 52.0, Fósforo: 39.0, Potasio: 39.0, Temperatura: 32.30°C, Humedad: 61.14%, pH: 6.55, Lluvia: 245.62 mm. ¿Qué cultivo recomiendas sembrar y por qué?\"\n",
        "\n",
        "inputs = tokenizer(test_example, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "outputs = model.generate(inputs[\"input_ids\"], max_length=500, num_return_sequences=1, attention_mask=inputs[\"attention_mask\"])\n",
        "\n",
        "predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(predicted_text)"
      ],
      "metadata": {
        "id": "W_gt0HI9rU-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimización para Uso Offline"
      ],
      "metadata": {
        "id": "F7f9jTXhMVg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Configuración de cuantización para optimización en 8 bits\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Usar 8 bits para optimizar la memoria\n",
        "    llm_int8_threshold=6.0,  # Umbral para activación de cuantización\n",
        "    llm_int8_enable_fp32_cpu_offload=True  # Habilitar offloading de FP32 en CPU\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./fine_tuned_model\", quantization_config=quant_config, device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "TJG4ofsrMqBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}